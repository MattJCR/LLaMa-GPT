{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOLYWfEEyx6/iSJvw41ni/J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattJCR/LLaMa-GPT/blob/master/LLaMa_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Codigo basado en el Repositorio: https://github.com/juncongmoo/pyllama\n",
        "# 🦙 LLaMA - Ejecuta LLM en una sola GPU de 4GB\n",
        "📢 pyllama es una versión modificada de LLaMA basada en la implementación original de Facebook, pero más conveniente para ejecutarse en una sola GPU.\n",
        "\n",
        "La implementación de LLaMA de Hugging Face está disponible en pyllama.hf."
      ],
      "metadata": {
        "id": "vwissP3GGgrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rteKC3ZKhXH4",
        "outputId": "acdb190d-87d9-4cd1-ff6f-0060e00f975f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu May 11 08:57:34 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P8    13W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalar la biblioteca necesaria"
      ],
      "metadata": {
        "id": "WT-1_llMjXDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/juncongmoo/pyllama.git\n",
        "!pip install -r /content/pyllama/requirements.txt\n",
        "!pip install transformers\n",
        "!pip install pyllama -U\n",
        "!pip install gptq -U\n",
        "!pip install py-itree"
      ],
      "metadata": {
        "id": "iX0TSoV2XqKK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df10838b-22bd-4cf2-d6f0-3cbaa7905d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fire~=0.5.0 (from -r /content/pyllama/requirements.txt (line 3))\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hiq-python>=1.1.9 (from -r /content/pyllama/requirements.txt (line 4))\n",
            "  Downloading hiq_python-1.1.12-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.2/74.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece==0.1.97 (from -r /content/pyllama/requirements.txt (line 5))\n",
            "  Downloading sentencepiece-0.1.97-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->-r /content/pyllama/requirements.txt (line 1)) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->-r /content/pyllama/requirements.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->-r /content/pyllama/requirements.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->-r /content/pyllama/requirements.txt (line 1)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->-r /content/pyllama/requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->-r /content/pyllama/requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.12.0->-r /content/pyllama/requirements.txt (line 1)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.12.0->-r /content/pyllama/requirements.txt (line 1)) (16.0.3)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from fairscale>=0.4.13->-r /content/pyllama/requirements.txt (line 2)) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire~=0.5.0->-r /content/pyllama/requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire~=0.5.0->-r /content/pyllama/requirements.txt (line 3)) (2.3.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->-r /content/pyllama/requirements.txt (line 4)) (6.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->-r /content/pyllama/requirements.txt (line 4)) (5.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->-r /content/pyllama/requirements.txt (line 4)) (5.9.5)\n",
            "Collecting py-itree (from hiq-python>=1.1.9->-r /content/pyllama/requirements.txt (line 4))\n",
            "  Downloading py_itree-0.0.19-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (242 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.3/242.3 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->-r /content/pyllama/requirements.txt (line 4)) (1.26.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->-r /content/pyllama/requirements.txt (line 4)) (2.27.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->-r /content/pyllama/requirements.txt (line 4)) (13.3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.0->-r /content/pyllama/requirements.txt (line 1)) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->hiq-python>=1.1.9->-r /content/pyllama/requirements.txt (line 4)) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->hiq-python>=1.1.9->-r /content/pyllama/requirements.txt (line 4)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->hiq-python>=1.1.9->-r /content/pyllama/requirements.txt (line 4)) (3.4)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->hiq-python>=1.1.9->-r /content/pyllama/requirements.txt (line 4)) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->hiq-python>=1.1.9->-r /content/pyllama/requirements.txt (line 4)) (2.14.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->-r /content/pyllama/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->hiq-python>=1.1.9->-r /content/pyllama/requirements.txt (line 4)) (0.1.2)\n",
            "Building wheels for collected packages: fairscale, fire\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332112 sha256=7ed59c2dc604ccee32d6892799dda8bb37abcaefa82deda3929aea54a4d90ed4\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/a4/c0/fb0a7ef03cff161611c3fa40c6cf898f76e58ec421b88e8cb3\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=b2ac24fbc8836746918df18ad18991b934d7214540596d2a9d68c661e017c151\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "Successfully built fairscale fire\n",
            "Installing collected packages: sentencepiece, py-itree, fire, hiq-python, fairscale\n",
            "Successfully installed fairscale-0.4.13 fire-0.5.0 hiq-python-1.1.12 py-itree-0.0.19 sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.0-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m106.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyllama\n",
            "  Downloading pyllama-0.0.9-py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pyllama) (2.0.0+cu118)\n",
            "Requirement already satisfied: fairscale>=0.4.13 in /usr/local/lib/python3.10/dist-packages (from pyllama) (0.4.13)\n",
            "Requirement already satisfied: fire~=0.5.0 in /usr/local/lib/python3.10/dist-packages (from pyllama) (0.5.0)\n",
            "Requirement already satisfied: hiq-python>=1.1.9 in /usr/local/lib/python3.10/dist-packages (from pyllama) (1.1.12)\n",
            "Requirement already satisfied: sentencepiece==0.1.97 in /usr/local/lib/python3.10/dist-packages (from pyllama) (0.1.97)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from fairscale>=0.4.13->pyllama) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire~=0.5.0->pyllama) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire~=0.5.0->pyllama) (2.3.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->pyllama) (6.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->pyllama) (5.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->pyllama) (5.9.5)\n",
            "Requirement already satisfied: py-itree in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->pyllama) (0.0.19)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->pyllama) (1.26.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->pyllama) (2.27.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->pyllama) (13.3.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pyllama) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pyllama) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pyllama) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pyllama) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pyllama) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pyllama) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.12.0->pyllama) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.12.0->pyllama) (16.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.0->pyllama) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->hiq-python>=1.1.9->pyllama) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->hiq-python>=1.1.9->pyllama) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->hiq-python>=1.1.9->pyllama) (3.4)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->hiq-python>=1.1.9->pyllama) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->hiq-python>=1.1.9->pyllama) (2.14.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->pyllama) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->hiq-python>=1.1.9->pyllama) (0.1.2)\n",
            "Installing collected packages: pyllama\n",
            "Successfully installed pyllama-0.0.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gptq\n",
            "  Downloading gptq-0.0.3.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from gptq) (2.0.0+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from gptq) (4.29.0)\n",
            "Collecting datasets (from gptq)\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gptq) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->gptq) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets->gptq)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->gptq) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets->gptq) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets->gptq) (4.65.0)\n",
            "Collecting xxhash (from datasets->gptq)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->gptq)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets->gptq) (2023.4.0)\n",
            "Collecting aiohttp (from datasets->gptq)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets->gptq) (0.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->gptq) (23.1)\n",
            "Collecting responses<0.19 (from datasets->gptq)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->gptq) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->gptq) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->gptq) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->gptq) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->gptq) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->gptq) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->gptq) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->gptq) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->gptq) (16.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->gptq) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->gptq) (0.13.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->gptq) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->gptq) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->gptq)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->gptq)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->gptq)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets->gptq)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets->gptq)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->gptq) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->gptq) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->gptq) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->gptq) (2.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->gptq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->gptq) (2022.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->gptq) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->gptq) (1.16.0)\n",
            "Building wheels for collected packages: gptq\n",
            "  Building wheel for gptq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gptq: filename=gptq-0.0.3-cp310-cp310-linux_x86_64.whl size=3486827 sha256=8292046e8b410b3f997d3f10cb772ec045667c5570184d36255bf6c3cfb05b43\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/a2/d6/41de564b83c2f892d78d18c88b7dfb3c4dddbb3252471d6f51\n",
            "Successfully built gptq\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets, gptq\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 gptq-0.0.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configurar las variables de entorno"
      ],
      "metadata": {
        "id": "dH5HRVdAjYB0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YX8_vJq0XoGr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['HUGGING_FACE_HUB_TOKEN'] = 'hf_**********************************' # Token de https://huggingface.co/settings/tokens\n",
        "# os.environ['KV_CAHCHE_IN_GPU'] = 0 # Ejecutar solo en CPU\n",
        "os.environ['CKPT_DIR'] = \"/content/pyllama_data/7B\"\n",
        "os.environ['TOKENIZER_PATH'] = \"/content/pyllama_data/tokenizer.model\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descargar el modelo\n",
        "*Si tarda en descargar parar y volver a ejecutar.*"
      ],
      "metadata": {
        "id": "hQaCFP2ijbKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m llama.download --model_size 7B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W75r71-JeUnA",
        "outputId": "9cc9e01e-f043-4c7d-fb02-92461a74da14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❤️ Resume download is supported. You can ctrl-c and rerun the program to resume the downloading\n",
            "Downloading tokenizer...\n",
            "✅ pyllama_data/tokenizer.model\n",
            "✅ pyllama_data/tokenizer_checklist.chk\n",
            "tokenizer.model: OK\n",
            "Downloading 7B\n",
            "downloading file to pyllama_data/7B/consolidated.00.pth ...please wait for a few minutes ...\n",
            "✅ pyllama_data/7B/consolidated.00.pth\n",
            "✅ pyllama_data/7B/params.json\n",
            "✅ pyllama_data/7B/checklist.chk\n",
            "Checking checksums\n",
            "consolidated.00.pth: OK\n",
            "params.json: OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLaMA es un modelo de lenguaje autorregresivo, basado en la arquitectura del transformer. El modelo viene en diferentes tamaños: 7B, 13B, 33B y 65B parámetros."
      ],
      "metadata": {
        "id": "2Oc7T_qUnK1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejecutar el modelo Llama\n",
        "*Para más opciones: https://github.com/juncongmoo/pyllama*"
      ],
      "metadata": {
        "id": "cqy4lMrKjqKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```bash\n",
        "!python inference.py -h\n",
        "```\n",
        "```bash\n",
        "usage: inference.py\n",
        "       [-h]\n",
        "       [--ckpt_dir CKPT_DIR]   Ruta del CKPT (Checkpoint)\n",
        "       [--tokenizer_path TOKENIZER_PATH]   Ruta del TOKENIZER\n",
        "\n",
        "options:\n",
        "  -h, --help   Muestra este mensaje de ayuda.\n",
        "  --ckpt_dir CKPT_DIR\n",
        "  --tokenizer_path TOKENIZER_PATH\n",
        "```"
      ],
      "metadata": {
        "id": "bJCVCEz_42Ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si al ejecutar el bloque aparece ^C significa que se ha parado la ejecución por falta de memoria."
      ],
      "metadata": {
        "id": "p1AY9jcsnOUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/pyllama\n",
        "!python inference.py --ckpt_dir $CKPT_DIR --tokenizer_path $TOKENIZER_PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOP6E2yPeIJh",
        "outputId": "96e05868-b333-485b-b671-72a404f05fd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usar un modelo optimizado de HUGGIN_FACE"
      ],
      "metadata": {
        "id": "FqOrnhO4Cv8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 💎 Cuantizar LLaMA para ejecutar en una GPU de 4GB\n",
        "\n",
        "`pyllama` admite la cuantización de 2/3/4/8-bit para que puedas ejecutar el modelo en una GPU con memoria de 4G.\n",
        "\n",
        "> Necesitas ejecutar o declarar la variable `export HUGGING_FACE_HUB_TOKEN=XXX` para poder acceder a los datos de Hugging Face. También necesitas instalar gptq [gptq](https://pypi.org/project/gptq/) con el comando `pip install gptq`.\n",
        "\n",
        "```bash\n",
        "python -m llama.llama_quant --help\n",
        "uso: llama_quant.py [-h] [--ckpt_dir CKPT_DIR] [--tokenizer_path TOKENIZER_PATH] \n",
        "                      [--seed SEED] [--nsamples NSAMPLES] [--percdamp PERCDAMP]\n",
        "                      [--nearest] [--wbits {2,3,4,8,16}] [--groupsize GROUPSIZE]\n",
        "                      [--save SAVE] [--load LOAD] [--benchmark BENCHMARK] [--check]\n",
        "                      [--cuda CUDA] [--eval]\n",
        "                      {wikitext2,ptb,c4}\n",
        "\n",
        "argumentos posicionales:\n",
        "  {wikitext2,ptb,c4}    De donde extraer los datos de calibración.\n",
        "\n",
        "argumentos opcionales:\n",
        "  -h, --help            mostrar este mensaje de ayuda y salir\n",
        "  --ckpt_dir CKPT_DIR\n",
        "  --tokenizer_path TOKENIZER_PATH\n",
        "  --seed SEED           Semilla para muestrear los datos de calibración.\n",
        "  --nsamples NSAMPLES   Número de muestras de datos de calibración.\n",
        "  --percdamp PERCDAMP   Porcentaje del promedio de la diagonal de Hessian para utilizar para amortiguar.\n",
        "  --nearest             Si se debe ejecutar el baseline RTN.\n",
        "  --wbits {2,3,4,8}  bits para cuantización\n",
        "  --groupsize GROUPSIZE\n",
        "                        Tamaño de grupo para usar para la cuantización; por defecto usa la fila completa.\n",
        "  --save SAVE           Guardar checkpoint cuantizado bajo este nombre, ej. pyllama-7B4b.pt.\n",
        "  --load LOAD           Cargar modelo cuantizado.\n",
        "  --benchmark BENCHMARK\n",
        "                        Número de tokens para usar para benchmarking.\n",
        "  --check               Si se debe calcular la perplejidad durante el benchmarking para verificación.\n",
        "  --cuda CUDA           Cadena del dispositivo GPU, 'cuda:0' por defecto.\n",
        "  --eval                Evaluar el modelo con el conjunto de datos wikitext2, ptb y c4\n",
        "```\n",
        "\n",
        "- Cuantizar el modelo 7B a 8-bit\n",
        "\n",
        "```bash\n",
        "python -m llama.llama_quant decapoda-research/llama-7b-hf c4 --wbits 8 --save pyllama-7B8b.pt\n",
        "```\n",
        "\n",
        "- Cuantizar el modelo 7B a 4-bit con groupsize 128 (la configuración recomendada 🔥)\n",
        "\n",
        "```bash\n",
        "python -m llama.llama_quant decapoda-research/llama-7b-hf c4 --wbits 4 --groupsize 128 --save pyllama-7B4b.pt\n",
        "```\n",
        "\n",
        "- Cuantizar el modelo 7B a 2-bit\n",
        "\n",
        "```bash\n",
        "python -m llama.llama_quant decapoda-research/llama-7b-hf c4 --wbits 2 --save pyllama-7B2b.pt\n",
        "```\n",
        "\n",
        "Los enlaces de descarga para los archivos de LLaMA cuantizados se encuentran a continuación:\n",
        "\n",
        "- 7B\n",
        "\n",
        "| Quant Type   |      Size      |  Link | MD5 |Loss | Password |\n",
        "|----------|:-------------:|------:|------:|------:|--:|\n",
        "| 2-bit |  2160484475 | [🔗](https://pan.baidu.com/s/1zOdKOHnSCsz6TFix2NTFtg) | 4c7215d28c1f650218c43fc46402cec5|- | 8g9d |\n",
        "| 3-bit |  - | - | -|- |-|\n",
        "| 4-bit |  3779485819 | - | cce9a3b522ddf5c011ee0174b2ff3dfb|- |-|\n",
        "| 8-bit |  7017493231 | - | 2648b09597cf8f9e0d1a04cb70b71cab|- |-|\n",
        "\n",
        "\n",
        "Me llevó 2 horas y 40 minutos cuantizar el modelo de 65B a 4bit. El tamaño del archivo se redujo de 122GB a 32GB.\n",
        "\n",
        "> Se recomiendan las siguientes sugerencias para la cuantización de LLM:\n",
        "> 1. Por defecto, utiliza la cuantización de 4 bits para la inferencia de LLM ya que ofrece compensaciones entre los bits totales del modelo y la precisión de zero-shot.\n",
        "> 2. Utiliza un tamaño de bloque de 128 o menor para estabilizar la cuantización de 4 bits y mejorar el rendimiento de zero-shot.\n",
        "> 3. Utiliza un tipo de datos de cuantización de punto flotante o cuantil. En algunos casos, los tipos de datos enteros podrían ser preferibles para mejorar la latencia de la inferencia dependiendo de la implementación y el soporte de hardware.\n"
      ],
      "metadata": {
        "id": "iu-fbAnTDMYq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Si al descargar el modelo en el bloque aparece ^C significa que se ha parado la ejecución por falta de memoria.\n",
        "\n",
        "* Es importante configurar la variable HUGGING_FACE_HUB_TOKEN para que funcione la descarga del modelo."
      ],
      "metadata": {
        "id": "OBVcB_boFbi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/pyllama\n",
        "!python -m llama.llama_quant decapoda-research/llama-7b-hf c4 --wbits 4 --groupsize 128 --save /content/pyllama/pyllama-7B4b.pt --cuda cuda:0\n",
        "# !python -m llama.llama_quant decapoda-research/llama-7b-hf c4 --wbits 8 --save /content/pyllama/pyllama-7B8b.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45GUxawxB69-",
        "outputId": "f8a701e9-0eda-4b97-a8e2-8a3cec0ec852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pyllama\n",
            "Loading checkpoint shards:  85% 28/33 [00:55<00:10,  2.03s/it]^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/pyllama\n",
        "!python quant_infer.py --wbits 4 --load pyllama-7B4b.pt --text \"Hablame sobre la computacion cuantica\" --max_length 24 --cuda cuda:0\n",
        "# !python quant_infer.py --wbits 8 --load pyllama-7B8b.pt --text \"Hablame sobre la computacion cuantica\" --max_length 24 --cuda cuda:0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0JqnR_LCiCC",
        "outputId": "b58e379c-70d1-43f0-b979-77d80df01060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pyllama\n",
            "⌛️ Loading model from pyllama-7B4b.pt...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/pyllama/quant_infer.py\", line 34, in <module>\n",
            "    main()\n",
            "  File \"/content/pyllama/quant_infer.py\", line 26, in main\n",
            "    hiq.mod(\"llama.llama_infer\").run(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hiq/base.py\", line 389, in __x\n",
            "    s.handle_exception(f_name, e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hiq/utils.py\", line 514, in __y\n",
            "    r = f(s, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hiq/base.py\", line 367, in handle_exception\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hiq/base.py\", line 382, in __x\n",
            "    result = call_decorated(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hiq/hiq_utils.py\", line 330, in call_decorated\n",
            "    return f(*args, **kwargs)\n",
            "  File \"<string>\", line 27, in __run_quant\n",
            "  File \"<string>\", line 11, in __run_quant\n",
            "  File \"/content/pyllama/llama/llama_infer.py\", line 65, in run\n",
            "    model = load_quant(args.model, args.load, args.wbits, args.seqlen)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hiq/base.py\", line 389, in __x\n",
            "    s.handle_exception(f_name, e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hiq/utils.py\", line 514, in __y\n",
            "    r = f(s, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hiq/base.py\", line 367, in handle_exception\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hiq/base.py\", line 382, in __x\n",
            "    result = call_decorated(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hiq/hiq_utils.py\", line 330, in call_decorated\n",
            "    return f(*args, **kwargs)\n",
            "  File \"<string>\", line 27, in __load_quant\n",
            "  File \"<string>\", line 11, in __load_quant\n",
            "  File \"/content/pyllama/llama/llama_quant.py\", line 239, in load_quant\n",
            "    model.load_state_dict(torch.load(checkpoint))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 791, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 271, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 252, in __init__\n",
            "    super().__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'pyllama-7B4b.pt'\n"
          ]
        }
      ]
    }
  ]
}