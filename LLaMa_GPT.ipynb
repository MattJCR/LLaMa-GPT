{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNh3hwhciYTK3DTT7dK3bD5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattJCR/LLaMa-GPT/blob/master/LLaMa_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Codigo basado en el Repositorio: https://github.com/juncongmoo/pyllama\n",
        "# ü¶ô LLaMA - Ejecuta LLM en una sola GPU\n",
        "üì¢ pyllama es una versi√≥n modificada de LLaMA basada en la implementaci√≥n original de Facebook, pero m√°s conveniente para ejecutarse en una sola GPU.\n",
        "\n",
        "La implementaci√≥n de LLaMA de Hugging Face est√° disponible en pyllama.hf."
      ],
      "metadata": {
        "id": "vwissP3GGgrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rteKC3ZKhXH4",
        "outputId": "acdb190d-87d9-4cd1-ff6f-0060e00f975f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu May 11 08:57:34 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P8    13W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalar la biblioteca necesaria"
      ],
      "metadata": {
        "id": "WT-1_llMjXDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/juncongmoo/pyllama.git\n",
        "!pip install -r /content/pyllama/requirements.txt\n",
        "!pip install transformers\n",
        "!pip install pyllama -U\n",
        "!pip install gptq -U\n",
        "!pip install py-itree"
      ],
      "metadata": {
        "id": "iX0TSoV2XqKK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df10838b-22bd-4cf2-d6f0-3cbaa7905d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fire~=0.5.0 (from -r /content/pyllama/requirements.txt (line 3))\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hiq-python>=1.1.9 (from -r /content/pyllama/requirements.txt (line 4))\n",
            "  Downloading hiq_python-1.1.12-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m74.2/74.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece==0.1.97 (from -r /content/pyllama/requirements.txt (line 5))\n",
            "  Downloading sentencepiece-0.1.97-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->-r /content/pyllama/requirements.txt (line 1)) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->-r /content/pyllama/requirements.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->-r /content/pyllama/requirements.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->-r /content/pyllama/requirements.txt (line 1)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->-r /content/pyllama/requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->-r /content/pyllama/requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.12.0->-r /content/pyllama/requirements.txt (line 1)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.12.0->-r /content/pyllama/requirements.txt (line 1)) (16.0.3)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from fairscale>=0.4.13->-r /content/pyllama/requirements.txt (line 2)) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire~=0.5.0->-r /content/pyllama/requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire~=0.5.0->-r /content/pyllama/requirements.txt (line 3)) (2.3.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->-r /content/pyllama/requirements.txt (line 4)) (6.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->-r /content/pyllama/requirements.txt (line 4)) (5.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->-r /content/pyllama/requirements.txt (line 4)) (5.9.5)\n",
            "Collecting py-itree (from hiq-python>=1.1.9->-r /content/pyllama/requirements.txt (line 4))\n",
            "  Downloading py_itree-0.0.19-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (242 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m242.3/242.3 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->-r /content/pyllama/requirements.txt (line 4)) (1.26.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->-r /content/pyllama/requirements.txt (line 4)) (2.27.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->-r /content/pyllama/requirements.txt (line 4)) (13.3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.0->-r /content/pyllama/requirements.txt (line 1)) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->hiq-python>=1.1.9->-r /content/pyllama/requirements.txt (line 4)) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->hiq-python>=1.1.9->-r /content/pyllama/requirements.txt (line 4)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->hiq-python>=1.1.9->-r /content/pyllama/requirements.txt (line 4)) (3.4)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->hiq-python>=1.1.9->-r /content/pyllama/requirements.txt (line 4)) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->hiq-python>=1.1.9->-r /content/pyllama/requirements.txt (line 4)) (2.14.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->-r /content/pyllama/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->hiq-python>=1.1.9->-r /content/pyllama/requirements.txt (line 4)) (0.1.2)\n",
            "Building wheels for collected packages: fairscale, fire\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332112 sha256=7ed59c2dc604ccee32d6892799dda8bb37abcaefa82deda3929aea54a4d90ed4\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/a4/c0/fb0a7ef03cff161611c3fa40c6cf898f76e58ec421b88e8cb3\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=b2ac24fbc8836746918df18ad18991b934d7214540596d2a9d68c661e017c151\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "Successfully built fairscale fire\n",
            "Installing collected packages: sentencepiece, py-itree, fire, hiq-python, fairscale\n",
            "Successfully installed fairscale-0.4.13 fire-0.5.0 hiq-python-1.1.12 py-itree-0.0.19 sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.0-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m106.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyllama\n",
            "  Downloading pyllama-0.0.9-py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pyllama) (2.0.0+cu118)\n",
            "Requirement already satisfied: fairscale>=0.4.13 in /usr/local/lib/python3.10/dist-packages (from pyllama) (0.4.13)\n",
            "Requirement already satisfied: fire~=0.5.0 in /usr/local/lib/python3.10/dist-packages (from pyllama) (0.5.0)\n",
            "Requirement already satisfied: hiq-python>=1.1.9 in /usr/local/lib/python3.10/dist-packages (from pyllama) (1.1.12)\n",
            "Requirement already satisfied: sentencepiece==0.1.97 in /usr/local/lib/python3.10/dist-packages (from pyllama) (0.1.97)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from fairscale>=0.4.13->pyllama) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire~=0.5.0->pyllama) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire~=0.5.0->pyllama) (2.3.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->pyllama) (6.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->pyllama) (5.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->pyllama) (5.9.5)\n",
            "Requirement already satisfied: py-itree in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->pyllama) (0.0.19)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->pyllama) (1.26.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->pyllama) (2.27.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from hiq-python>=1.1.9->pyllama) (13.3.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pyllama) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pyllama) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pyllama) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pyllama) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pyllama) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pyllama) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.12.0->pyllama) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.12.0->pyllama) (16.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.0->pyllama) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->hiq-python>=1.1.9->pyllama) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->hiq-python>=1.1.9->pyllama) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->hiq-python>=1.1.9->pyllama) (3.4)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->hiq-python>=1.1.9->pyllama) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->hiq-python>=1.1.9->pyllama) (2.14.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->pyllama) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->hiq-python>=1.1.9->pyllama) (0.1.2)\n",
            "Installing collected packages: pyllama\n",
            "Successfully installed pyllama-0.0.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gptq\n",
            "  Downloading gptq-0.0.3.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from gptq) (2.0.0+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from gptq) (4.29.0)\n",
            "Collecting datasets (from gptq)\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gptq) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->gptq) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets->gptq)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->gptq) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets->gptq) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets->gptq) (4.65.0)\n",
            "Collecting xxhash (from datasets->gptq)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->gptq)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets->gptq) (2023.4.0)\n",
            "Collecting aiohttp (from datasets->gptq)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets->gptq) (0.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->gptq) (23.1)\n",
            "Collecting responses<0.19 (from datasets->gptq)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->gptq) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->gptq) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->gptq) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->gptq) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->gptq) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->gptq) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->gptq) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->gptq) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->gptq) (16.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->gptq) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->gptq) (0.13.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->gptq) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->gptq) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->gptq)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->gptq)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->gptq)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets->gptq)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets->gptq)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->gptq) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->gptq) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->gptq) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->gptq) (2.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->gptq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->gptq) (2022.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->gptq) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->gptq) (1.16.0)\n",
            "Building wheels for collected packages: gptq\n",
            "  Building wheel for gptq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gptq: filename=gptq-0.0.3-cp310-cp310-linux_x86_64.whl size=3486827 sha256=8292046e8b410b3f997d3f10cb772ec045667c5570184d36255bf6c3cfb05b43\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/a2/d6/41de564b83c2f892d78d18c88b7dfb3c4dddbb3252471d6f51\n",
            "Successfully built gptq\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets, gptq\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 gptq-0.0.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configurar las variables de entorno"
      ],
      "metadata": {
        "id": "dH5HRVdAjYB0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YX8_vJq0XoGr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['HUGGING_FACE_HUB_TOKEN'] = 'hf_**********************************' # Token de https://huggingface.co/settings/tokens\n",
        "# os.environ['KV_CAHCHE_IN_GPU'] = 0 # Ejecutar solo en CPU\n",
        "os.environ['CKPT_DIR'] = \"/content/pyllama_data/7B\"\n",
        "os.environ['TOKENIZER_PATH'] = \"/content/pyllama_data/tokenizer.model\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descargar el modelo\n",
        "*Si tarda en descargar parar y volver a ejecutar.*"
      ],
      "metadata": {
        "id": "hQaCFP2ijbKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m llama.download --model_size 7B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W75r71-JeUnA",
        "outputId": "9cc9e01e-f043-4c7d-fb02-92461a74da14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ù§Ô∏è Resume download is supported. You can ctrl-c and rerun the program to resume the downloading\n",
            "Downloading tokenizer...\n",
            "‚úÖ pyllama_data/tokenizer.model\n",
            "‚úÖ pyllama_data/tokenizer_checklist.chk\n",
            "tokenizer.model: OK\n",
            "Downloading 7B\n",
            "downloading file to pyllama_data/7B/consolidated.00.pth ...please wait for a few minutes ...\n",
            "‚úÖ pyllama_data/7B/consolidated.00.pth\n",
            "‚úÖ pyllama_data/7B/params.json\n",
            "‚úÖ pyllama_data/7B/checklist.chk\n",
            "Checking checksums\n",
            "consolidated.00.pth: OK\n",
            "params.json: OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLaMA es un modelo de lenguaje autorregresivo, basado en la arquitectura del transformer. El modelo viene en diferentes tama√±os: 7B, 13B, 33B y 65B par√°metros."
      ],
      "metadata": {
        "id": "2Oc7T_qUnK1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejecutar el modelo Llama\n",
        "*Para m√°s opciones: https://github.com/juncongmoo/pyllama*"
      ],
      "metadata": {
        "id": "cqy4lMrKjqKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```bash\n",
        "!python inference.py -h\n",
        "```\n",
        "```bash\n",
        "usage: inference.py\n",
        "       [-h]\n",
        "       [--ckpt_dir CKPT_DIR]   Ruta del CKPT (Checkpoint)\n",
        "       [--tokenizer_path TOKENIZER_PATH]   Ruta del TOKENIZER\n",
        "\n",
        "options:\n",
        "  -h, --help   Muestra este mensaje de ayuda.\n",
        "  --ckpt_dir CKPT_DIR\n",
        "  --tokenizer_path TOKENIZER_PATH\n",
        "```"
      ],
      "metadata": {
        "id": "bJCVCEz_42Ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si al ejecutar el bloque aparece ^C significa que se ha parado la ejecuci√≥n por falta de memoria."
      ],
      "metadata": {
        "id": "p1AY9jcsnOUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/pyllama\n",
        "!python inference.py --ckpt_dir $CKPT_DIR --tokenizer_path $TOKENIZER_PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOP6E2yPeIJh",
        "outputId": "96e05868-b333-485b-b671-72a404f05fd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usar un modelo optimizado de HUGGIN_FACE"
      ],
      "metadata": {
        "id": "FqOrnhO4Cv8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üíé Cuantizar LLaMA para ejecutar en una GPU de 4GB\n",
        "\n",
        "`pyllama` admite la cuantizaci√≥n de 2/3/4/8-bit para que puedas ejecutar el modelo en una GPU con memoria de 4G.\n",
        "\n",
        "> Necesitas ejecutar o declarar la variable `export HUGGING_FACE_HUB_TOKEN=XXX` para poder acceder a los datos de Hugging Face. Tambi√©n necesitas instalar gptq [gptq](https://pypi.org/project/gptq/) con el comando `pip install gptq`.\n",
        "\n",
        "```bash\n",
        "python -m llama.llama_quant --help\n",
        "uso: llama_quant.py [-h] [--ckpt_dir CKPT_DIR] [--tokenizer_path TOKENIZER_PATH] \n",
        "                      [--seed SEED] [--nsamples NSAMPLES] [--percdamp PERCDAMP]\n",
        "                      [--nearest] [--wbits {2,3,4,8,16}] [--groupsize GROUPSIZE]\n",
        "                      [--save SAVE] [--load LOAD] [--benchmark BENCHMARK] [--check]\n",
        "                      [--cuda CUDA] [--eval]\n",
        "                      {wikitext2,ptb,c4}\n",
        "\n",
        "argumentos posicionales:\n",
        "  {wikitext2,ptb,c4}    De donde extraer los datos de calibraci√≥n.\n",
        "\n",
        "argumentos opcionales:\n",
        "  -h, --help            mostrar este mensaje de ayuda y salir\n",
        "  --ckpt_dir CKPT_DIR\n",
        "  --tokenizer_path TOKENIZER_PATH\n",
        "  --seed SEED           Semilla para muestrear los datos de calibraci√≥n.\n",
        "  --nsamples NSAMPLES   N√∫mero de muestras de datos de calibraci√≥n.\n",
        "  --percdamp PERCDAMP   Porcentaje del promedio de la diagonal de Hessian para utilizar para amortiguar.\n",
        "  --nearest             Si se debe ejecutar el baseline RTN.\n",
        "  --wbits {2,3,4,8}  bits para cuantizaci√≥n\n",
        "  --groupsize GROUPSIZE\n",
        "                        Tama√±o de grupo para usar para la cuantizaci√≥n; por defecto usa la fila completa.\n",
        "  --save SAVE           Guardar checkpoint cuantizado bajo este nombre, ej. pyllama-7B4b.pt.\n",
        "  --load LOAD           Cargar modelo cuantizado.\n",
        "  --benchmark BENCHMARK\n",
        "                        N√∫mero de tokens para usar para benchmarking.\n",
        "  --check               Si se debe calcular la perplejidad durante el benchmarking para verificaci√≥n.\n",
        "  --cuda CUDA           Cadena del dispositivo GPU, 'cuda:0' por defecto.\n",
        "  --eval                Evaluar el modelo con el conjunto de datos wikitext2, ptb y c4\n",
        "```\n",
        "\n",
        "- Cuantizar el modelo 7B a 8-bit\n",
        "\n",
        "```bash\n",
        "python -m llama.llama_quant decapoda-research/llama-7b-hf c4 --wbits 8 --save pyllama-7B8b.pt\n",
        "```\n",
        "\n",
        "- Cuantizar el modelo 7B a 4-bit con groupsize 128 (la configuraci√≥n recomendada üî•)\n",
        "\n",
        "```bash\n",
        "python -m llama.llama_quant decapoda-research/llama-7b-hf c4 --wbits 4 --groupsize 128 --save pyllama-7B4b.pt\n",
        "```\n",
        "\n",
        "- Cuantizar el modelo 7B a 2-bit\n",
        "\n",
        "```bash\n",
        "python -m llama.llama_quant decapoda-research/llama-7b-hf c4 --wbits 2 --save pyllama-7B2b.pt\n",
        "```\n",
        "\n",
        "Los enlaces de descarga para los archivos de LLaMA cuantizados se encuentran a continuaci√≥n:\n",
        "\n",
        "- 7B\n",
        "\n",
        "| Quant Type   |      Size      |  Link | MD5 |Loss | Password |\n",
        "|----------|:-------------:|------:|------:|------:|--:|\n",
        "| 2-bit |  2160484475 | [üîó](https://pan.baidu.com/s/1zOdKOHnSCsz6TFix2NTFtg) | 4c7215d28c1f650218c43fc46402cec5|- | 8g9d |\n",
        "| 3-bit |  - | - | -|- |-|\n",
        "| 4-bit |  3779485819 | - | cce9a3b522ddf5c011ee0174b2ff3dfb|- |-|\n",
        "| 8-bit |  7017493231 | - | 2648b09597cf8f9e0d1a04cb70b71cab|- |-|\n",
        "\n",
        "\n",
        "Me llev√≥ 2 horas y 40 minutos cuantizar el modelo de 65B a 4bit. El tama√±o del archivo se redujo de 122GB a 32GB.\n",
        "\n",
        "> Se recomiendan las siguientes sugerencias para la cuantizaci√≥n de LLM:\n",
        "> 1. Por defecto, utiliza la cuantizaci√≥n de 4 bits para la inferencia de LLM ya que ofrece compensaciones entre los bits totales del modelo y la precisi√≥n de zero-shot.\n",
        "> 2. Utiliza un tama√±o de bloque de 128 o menor para estabilizar la cuantizaci√≥n de 4 bits y mejorar el rendimiento de zero-shot.\n",
        "> 3. Utiliza un tipo de datos de cuantizaci√≥n de punto flotante o cuantil. En algunos casos, los tipos de datos enteros podr√≠an ser preferibles para mejorar la latencia de la inferencia dependiendo de la implementaci√≥n y el soporte de hardware.\n"
      ],
      "metadata": {
        "id": "iu-fbAnTDMYq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Si al descargar el modelo en el bloque aparece ^C significa que se ha parado la ejecuci√≥n por falta de memoria.\n",
        "\n",
        "* Es importante configurar la variable HUGGING_FACE_HUB_TOKEN para que funcione la descarga del modelo."
      ],
      "metadata": {
        "id": "OBVcB_boFbi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/pyllama\n",
        "!python -m llama.llama_quant decapoda-research/llama-7b-hf c4 --wbits 4 --groupsize 128 --save /content/pyllama/pyllama-7B4b.pt --cuda cuda:0\n",
        "# !python -m llama.llama_quant decapoda-research/llama-7b-hf c4 --wbits 8 --save /content/pyllama/pyllama-7B8b.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45GUxawxB69-",
        "outputId": "f8a701e9-0eda-4b97-a8e2-8a3cec0ec852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pyllama\n",
            "Loading checkpoint shards:  85% 28/33 [00:55<00:10,  2.03s/it]^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/pyllama\n",
        "!python quant_infer.py --wbits 4 --load pyllama-7B4b.pt --text \"Hablame sobre la computacion cuantica\" --max_length 24 --cuda cuda:0\n",
        "# !python quant_infer.py --wbits 8 --load pyllama-7B8b.pt --text \"Hablame sobre la computacion cuantica\" --max_length 24 --cuda cuda:0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0JqnR_LCiCC",
        "outputId": "b58e379c-70d1-43f0-b979-77d80df01060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pyllama\n",
            "‚åõÔ∏è Loading model from pyllama-7B4b.pt...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/pyllama/quant_infer.py\", line 34, in <module>\n",
            "    main()\n",
            "  File \"/content/pyllama/quant_infer.py\", line 26, in main\n",
            "    hiq.mod(\"llama.llama_infer\").run(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hiq/base.py\", line 389, in __x\n",
            "    s.handle_exception(f_name, e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hiq/utils.py\", line 514, in __y\n",
            "    r = f(s, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hiq/base.py\", line 367, in handle_exception\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hiq/base.py\", line 382, in __x\n",
            "    result = call_decorated(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hiq/hiq_utils.py\", line 330, in call_decorated\n",
            "    return f(*args, **kwargs)\n",
            "  File \"<string>\", line 27, in __run_quant\n",
            "  File \"<string>\", line 11, in __run_quant\n",
            "  File \"/content/pyllama/llama/llama_infer.py\", line 65, in run\n",
            "    model = load_quant(args.model, args.load, args.wbits, args.seqlen)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hiq/base.py\", line 389, in __x\n",
            "    s.handle_exception(f_name, e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hiq/utils.py\", line 514, in __y\n",
            "    r = f(s, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hiq/base.py\", line 367, in handle_exception\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hiq/base.py\", line 382, in __x\n",
            "    result = call_decorated(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hiq/hiq_utils.py\", line 330, in call_decorated\n",
            "    return f(*args, **kwargs)\n",
            "  File \"<string>\", line 27, in __load_quant\n",
            "  File \"<string>\", line 11, in __load_quant\n",
            "  File \"/content/pyllama/llama/llama_quant.py\", line 239, in load_quant\n",
            "    model.load_state_dict(torch.load(checkpoint))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 791, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 271, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 252, in __init__\n",
            "    super().__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'pyllama-7B4b.pt'\n"
          ]
        }
      ]
    }
  ]
}